{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bbb0cc76",
      "metadata": {
        "id": "bbb0cc76"
      },
      "source": [
        "# Steps\n",
        "1. Import data\n",
        "2. Data Preprocessing: convert data into a `spectogram`\n",
        "3. Data Splitting: split data into train, dev, and test sets\n",
        "4. Create Model\n",
        "5. Train Model\n",
        "6. Evaluate Results\n",
        "\n",
        "## Libraries\n",
        "1. Librosa: Python package for music and audio analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "!pip install --upgrade wandb\n",
        "!pip install --upgrade tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnBJXnCgLqJ7",
        "outputId": "98ebc688-67e7-4077-b0f6-797fed912b7a"
      },
      "id": "TnBJXnCgLqJ7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 8.2 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 66.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 74.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 64.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 71.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 59.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 63.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 65.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 69.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 66.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 70.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f6d73f6c34b12d14a1427cd0aa805207e0a971b395212a0681c8a4de36ae1c5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 588.3 MB 6.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Collecting tensorflow_datasets\n",
            "  Downloading tensorflow_datasets-4.7.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 67.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.64.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.10.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.10.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.6)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.4)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow-datasets, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.6.0\n",
            "    Uninstalling tensorflow-datasets-4.6.0:\n",
            "      Successfully uninstalled tensorflow-datasets-4.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-22.10.26 keras-2.11.0 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-datasets-4.7.0 tensorflow-estimator-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "338d2a5d",
      "metadata": {
        "id": "338d2a5d"
      },
      "outputs": [],
      "source": [
        "# from preprocess import *\n",
        "import keras\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython import display\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "zMmYjNCXLgJm"
      },
      "id": "zMmYjNCXLgJm"
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = 'data/mini_speech_commands'\n",
        "\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdU5m_o0Li5l",
        "outputId": "c591ff60-577f-4da8-8783-b675ba24ae54"
      },
      "id": "cdU5m_o0Li5l",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\n",
            "182082353/182082353 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fd3e69",
      "metadata": {
        "id": "06fd3e69"
      },
      "source": [
        "## Check Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e70ba547",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "e70ba547",
        "outputId": "12a5876e-dc7a-4ac9-b231-2fcec264907f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        }
      ],
      "source": [
        "wandb.init()\n",
        "config = wandb.config\n",
        "\n",
        "# constants\n",
        "config.autotune = tf.data.AUTOTUNE\n",
        "\n",
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[commands != 'README.md']\n",
        "print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05eb053",
      "metadata": {
        "id": "f05eb053"
      },
      "source": [
        "## Data Splitting\n",
        "Using `tf.keras.utils.audio_dataset_from_directory` which do the heavy lifting for us in splitting the data. The output function will be a `tf.data.Dataset` based on the audio files where we can implement parallellization, prefecthing and caching for faster training.\n",
        "\n",
        "The arguments that we need to provide are:\n",
        "1. Batch size\n",
        "2. Validation split\n",
        "3. Output sequence length: maximum length of audio sequence. If not being set then all audio files will be set to the longest audio sequence and will be zero padded.\n",
        "4. Subset : subset of data return whether it is 'training', 'validation', and 'both'. 'both' can be run if **validation_split** parameter is set to **True**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91abf396",
      "metadata": {
        "id": "91abf396"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset='both',\n",
        ")\n",
        "\n",
        "label_names = np.array(train_ds.class_names)\n",
        "print()\n",
        "print(\"label names:\", label_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa002d9",
      "metadata": {
        "id": "dfa002d9"
      },
      "outputs": [],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f62c09",
      "metadata": {
        "id": "b8f62c09"
      },
      "source": [
        "Since the dataset only contains only a single channels then we can delete the last axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc694a84",
      "metadata": {
        "id": "bc694a84"
      },
      "outputs": [],
      "source": [
        "def squeeze(audio, labels):\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    return audio, labels\n",
        "\n",
        "train_ds = train_ds.map(squeeze, num_parallel_calls=config.autotune)\n",
        "val_ds = val_ds.map(squeeze, num_parallel_calls=config.autotune)\n",
        "print(train_ds.element_spec)\n",
        "print(val_ds.element_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074ef206",
      "metadata": {
        "id": "074ef206"
      },
      "source": [
        "Since the `tf.utils.audio_dataset_from_directory` function only return train and validation sets (two splits). It is a good idea to keep a test set separate and we can do that using the `Dataset.shard`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee68a1c7",
      "metadata": {
        "id": "ee68a1c7"
      },
      "outputs": [],
      "source": [
        "# example [0,1,2,3,4,5,6,7,9]\n",
        "# test_ds = val_ds.shard(num_shards=2, index=0) -> 0, 2, 4, 6, 8\n",
        "# val_ds = val_ds.shard(num_shards=2, index=1) -> 1, 3, 5, 7, 9\n",
        "test_ds = val_ds.shard(num_shards=2, index=0)\n",
        "val_ds = val_ds.shard(num_shards=2, index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9afd3d",
      "metadata": {
        "id": "9c9afd3d"
      },
      "outputs": [],
      "source": [
        "for example_audio, example_labels in train_ds.take(1):\n",
        "    print(\"Audio example shape: {}\".format(example_audio.shape))\n",
        "    print(\"Audio example label: {}\".format(example_labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c6a8da",
      "metadata": {
        "id": "d4c6a8da"
      },
      "source": [
        "## Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b041e4",
      "metadata": {
        "id": "d1b041e4"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(18, 12))\n",
        "\n",
        "for i in range(n):\n",
        "    if i>=n:\n",
        "        break\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    ax.plot(example_audio[i].numpy())\n",
        "    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "    label = label_names[example_labels[i]]\n",
        "    ax.set_title(label)\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105e59ce",
      "metadata": {
        "id": "105e59ce"
      },
      "source": [
        "## Data Preprocessing\n",
        "Converting waveforms to **spectograms** (x-axis=time, y-axis=frequency) using **short-time Fourier transform (STFT)**. **Spectograms** which show frequency changes over time and can be represented as 2D images. This spectogram is what will be feed to the neural network model.\n",
        "\n",
        "A **Fourier transform** (tf.signal.fft) and **STFT** (tf.signal.stft) both converts a signal to its component **frequencies**. The difference is a **Fourier Transform** loses all time information but the **STFT** (**divide the waveforms into multiple windows of time and runs a Fourier Transform** on it) preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n",
        "![image.png](https://github.com/marcellinus-witarsah/speech-to-text-model/blob/main/images/spectogram.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db17d861",
      "metadata": {
        "id": "db17d861"
      },
      "source": [
        "Create a utility function for converting waveforms to spectograms:\n",
        "1. The waveforms need to be the same length because we want the same length spectogram. If one of the audio waveform length is shorter than the longest length in the data, then it will be padded by 0 filling the remaining length.\n",
        "2. When calling `tf.signal.stft`, choose the `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on the STFT parameters choice.\n",
        "3. The STFT produces an array of complex numbers representing magnitude and phase. However, in this tutorial you'll only use the magnitude, which you can derive by applying `tf.abs` on the output of `tf.signal.stft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01a924d",
      "metadata": {
        "id": "a01a924d"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "    spectogram = tf.signal.stft(\n",
        "        waveform, frame_length=255, frame_step=128,\n",
        "    )\n",
        "    spectogram=tf.abs(spectogram)\n",
        "    spectogram=spectogram[..., tf.newaxis]\n",
        "    return spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cce2e11",
      "metadata": {
        "id": "6cce2e11"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    label = commands[example_labels[i]]\n",
        "    waveform = example_audio[i]\n",
        "    spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "    print('Label:', label)\n",
        "    print('Waveform shape:', waveform.shape)\n",
        "    print('Spectrogram shape:', spectrogram.shape)\n",
        "    print('Audio playback')\n",
        "    display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7acb580",
      "metadata": {
        "id": "f7acb580"
      },
      "source": [
        "Plot spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d35143c",
      "metadata": {
        "id": "4d35143c"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "    if len(spectrogram.shape) > 2:\n",
        "        assert len(spectrogram.shape) == 3\n",
        "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "    # Convert the frequencies to log scale and transpose, so that the time is\n",
        "    # represented on the x-axis (columns).\n",
        "    # Add an epsilon to avoid taking a log of zero.\n",
        "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "    height = log_spec.shape[0]\n",
        "    width = log_spec.shape[1]\n",
        "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "    Y = range(height)\n",
        "    ax.pcolormesh(X, Y, log_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ef307e",
      "metadata": {
        "id": "46ef307e"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.suptitle(label.title())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create spectrogram from the audio datasets"
      ],
      "metadata": {
        "id": "NJFBrrXpNe5G"
      },
      "id": "NJFBrrXpNe5G"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_spectrogram(ds):\n",
        "    return ds.map(lambda audio, label: (get_spectrogram(audio), label),\n",
        "                  num_parallel_calls=config.autotune)"
      ],
      "metadata": {
        "id": "_CO4O9cyNkBt"
      },
      "id": "_CO4O9cyNkBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_spectrogram_ds = make_spectrogram(train_ds)\n",
        "val_spectrogram_ds = make_spectrogram(val_ds)\n",
        "test_spectrogram_ds = make_spectrogram(test_ds)\n"
      ],
      "metadata": {
        "id": "8SaIw-lOOPYl"
      },
      "id": "8SaIw-lOOPYl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
        "  break"
      ],
      "metadata": {
        "id": "cSZoKQrvOaDz"
      },
      "id": "cSZoKQrvOaDz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
        "    ax.set_title(commands[example_spect_labels[i].numpy()])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eZz91tmqQOpe"
      },
      "id": "eZz91tmqQOpe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model"
      ],
      "metadata": {
        "id": "ghpR_64uQVlh"
      },
      "id": "ghpR_64uQVlh"
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "train_spectrogram_ds = train_spectrogram_ds.shuffle(10000).prefetch(config.autotune)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(config.autotune)\n",
        "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(config.autotune)"
      ],
      "metadata": {
        "id": "E0Dc3eGtVbA0"
      },
      "id": "E0Dc3eGtVbA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input shape\n",
        "input_shape = example_spectrograms.shape[1:]\n",
        "print(\"input shape: {}\".format(input_shape))\n",
        "num_labels = len(commands)\n",
        "\n",
        "# Normalize layer\n",
        "norm_layer = tf.keras.layers.Normalization()\n",
        "# Normalizaiton layer according to the data that is available\n",
        "norm_layer.adapt(data=train_ds.map(lambda spec, label: spec))\n",
        "\n",
        "# CNN Model\n",
        "def cnn_model(input_shape):\n",
        "    return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape),\n",
        "        # resize\n",
        "        tf.keras.layers.Resizing(32, 32),\n",
        "        # normalize\n",
        "        norm_layer,\n",
        "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
        "        tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(num_labels)\n",
        "    ])\n",
        "model = cnn_model(input_shape)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zrgWZBsLQgMp"
      },
      "id": "zrgWZBsLQgMp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "LGUj9Jo_Q_rS"
      },
      "id": "LGUj9Jo_Q_rS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(verbose=1, patience=2)],\n",
        ")"
      ],
      "metadata": {
        "id": "nnMzZRJrUrWL"
      },
      "id": "nnMzZRJrUrWL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9sB22j7JVARu"
      },
      "id": "9sB22j7JVARu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "speech-recognition (ipykernel)",
      "language": "python",
      "name": "speech-reocgnition"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}